/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.spark.ml.made

import scala.math.{signum, sqrt}

import scala.util.Random
import org.apache.hadoop.fs.Path
import org.apache.spark.annotation.Since
import org.apache.spark.ml.feature.{LSH, LSHModel}
import org.apache.spark.ml.linalg.{Vector, VectorUDT, Vectors}
import org.apache.spark.ml.param.ParamMap
import org.apache.spark.ml.param.shared.HasSeed
import org.apache.spark.ml.util._
import org.apache.spark.sql.types.StructType

/**
 * Model produced by [[MinHashLSH]], where multiple hash functions are stored. Each hash function
 * is picked from the following family of hash functions, where a_i and b_i are randomly chosen
 * integers less than prime:
 *    `h_i(x) = ((x \cdot a_i + b_i) \mod prime)`
 *
 * This hash family is approximately min-wise independent according to the reference.
 *
 * Reference:
 * Tom Bohman, Colin Cooper, and Alan Frieze. "Min-wise independent linear permutations."
 * Electronic Journal of Combinatorics 7 (2000): R26.
 *
 * @param randCoefficients Pairs of random coefficients. Each pair is used by one hash function.
 */
@Since("2.1.0")
class RandomHyperplanesLSHModel private[ml](
                                   override val uid: String,
                                   private[ml] val randHyperplanes: Array[Vector])
  extends LSHModel[RandomHyperplanesLSHModel] {

  /** @group setParam */
  @Since("2.4.0")
  override def setInputCol(value: String): this.type = super.set(inputCol, value)

  /** @group setParam */
  @Since("2.4.0")
  override def setOutputCol(value: String): this.type = super.set(outputCol, value)

  @Since("2.1.0")
  override protected[ml] def hashFunction(elems: Vector): Array[Vector] = {
    require(elems.nonZeroIterator.nonEmpty, "Must have at least 1 non zero entry.")
    val hashValues = randHyperplanes.map { case plane =>
      signum(
        elems.nonZeroIterator.map { case (i, v) =>
          v * plane(i)
        }.sum
      )
    }
    hashValues.map(Vectors.dense(_))
  }

  @Since("2.1.0")
  override protected[ml] def keyDistance(x: Vector, y: Vector): Double = {
    val x_iter = x.nonZeroIterator.map(_._1)
    val y_iter = y.nonZeroIterator.map(_._1)
    if (x_iter.isEmpty) {
      require(y_iter.hasNext, "min 1 elem")
      return 1.0
    } else if (y_iter.isEmpty) {
      return 1.0
    }
    var x_pow2:Double = 0.0
    var y_pow2:Double = 0.0
    var xy:Double = 0.0

    var x_index = x_iter.next
    var y_index = y_iter.next
    while (x_index!= -1 && y_index!= -1) {
      if (x_index == y_index) {
        val x_tmp = x(x_index)
        val y_tmp = y(y_index)
        xy += x_tmp*y_tmp
        x_pow2 += x_tmp*x_tmp
        y_pow2 += y_tmp*y_tmp
        if (x_iter.hasNext && y_iter.hasNext) {
          x_index = x_iter.next
          y_index = y_iter.next
        } else {
          x_index = -1
          y_index = -1
        }
      } else if (x_index > y_index) {
        val y_tmp = y(y_index)
        y_pow2 += y_tmp*y_tmp
        if (y_iter.hasNext) {
          y_index = y_iter.next
        } else {
          y_index = -1
        }
      } else {
        val x_tmp = x(x_index)
        x_pow2 += x_tmp*x_tmp
        if (x_iter.hasNext) {
          x_index = x_iter.next
        } else {
          x_index = -1
        }
      }
    }
    require(x_pow2*y_pow2 > 0, "zero devision")
    val cos_aprox = xy / sqrt(x_pow2*y_pow2)
    1 - cos_aprox
  }
// pause
  @Since("2.1.0")
  override protected[ml] def hashDistance(x: Seq[Vector], y: Seq[Vector]): Double = {
    // Since it's generated by hashing, it will be a pair of dense vectors.
    // TODO: This hashDistance function requires more discussion in SPARK-18454
    x.iterator.zip(y.iterator).map(vectorPair =>
      vectorPair._1.toArray.zip(vectorPair._2.toArray).count(pair => pair._1 != pair._2)
    ).min
  }

  @Since("2.1.0")
  override def copy(extra: ParamMap): RandomHyperplanesLSHModel = {
    val copied = new RandomHyperplanesLSHModel(uid, randHyperplanes).setParent(parent)
    copyValues(copied, extra)
  }

  @Since("2.1.0")
  override def write: MLWriter = new RandomHyperplanesLSHModel.RandomHyperplanesLSHModelWriter(this)

  @Since("3.0.0")
  override def toString: String = {
    s"RandomHyperplanesLSHModel: uid=$uid, numHashTables=${$(numHashTables)}"
  }

}


/**
 * LSH class for Jaccard distance.
 *
 * The input can be dense or sparse vectors, but it is more efficient if it is sparse. For example,
 *    `Vectors.sparse(10, Array((2, 1.0), (3, 1.0), (5, 1.0)))`
 * means there are 10 elements in the space. This set contains elements 2, 3, and 5. Also, any
 * input vector must have at least 1 non-zero index, and all non-zero values are
 * treated as binary "1" values.
 *
 * References:
 * <a href="https://en.wikipedia.org/wiki/MinHash">Wikipedia on MinHash</a>
 */
@Since("2.1.0")
class RandomHyperplanesLSH(override val uid: String) extends LSH[RandomHyperplanesLSHModel] with HasSeed {

  @Since("2.1.0")
  override def setInputCol(value: String): this.type = super.setInputCol(value)

  @Since("2.1.0")
  override def setOutputCol(value: String): this.type = super.setOutputCol(value)

  @Since("2.1.0")
  override def setNumHashTables(value: Int): this.type = super.setNumHashTables(value)

  @Since("2.1.0")
  def this() = {
    this(Identifiable.randomUID("rhp-lsh"))
  }

  /** @group setParam */
  @Since("2.1.0")
  def setSeed(value: Long): this.type = set(seed, value)

  @Since("2.1.0")
  override protected[ml] def createRawLSHModel(dim: Int): RandomHyperplanesLSHModel = {
    val seed_planes = new Random($(seed))
    val random_planes: Array[Vector] = Array.fill($(numHashTables)) {
      Vectors.dense(Array.fill(dim)(2*seed_planes.nextDouble - 1))
    }
    new RandomHyperplanesLSHModel(uid, random_planes)
  }


  @Since("2.1.0")
  override def transformSchema(schema: StructType): StructType = {
    SchemaUtils.checkColumnType(schema, $(inputCol), new VectorUDT)
    validateAndTransformSchema(schema)
  }

  @Since("2.1.0")
  override def copy(extra: ParamMap): this.type = defaultCopy(extra)
}


@Since("2.1.0")
object RandomHyperplanesLSHModel extends MLReadable[RandomHyperplanesLSHModel] {

  @Since("2.1.0")
  override def read: MLReader[RandomHyperplanesLSHModel] = new RandomHyperplanesLSHModelReader

  @Since("2.1.0")
  override def load(path: String): RandomHyperplanesLSHModel = super.load(path)

  private[RandomHyperplanesLSHModel] class RandomHyperplanesLSHModelWriter(instance: RandomHyperplanesLSHModel)
    extends MLWriter {

    private case class Data(random_planes: Array[Vector])

    override protected def saveImpl(path: String): Unit = {
      DefaultParamsWriter.saveMetadata(instance, path, sc)
      val data = Data(instance.randHyperplanes)
      val dataPath = new Path(path, "data").toString
      sparkSession.createDataFrame(Seq(data)).repartition(1).write.parquet(dataPath)
    }
  }



  private class RandomHyperplanesLSHModelReader extends MLReader[RandomHyperplanesLSHModel] {

    /** Checked against metadata when loading model */
    private val className = classOf[RandomHyperplanesLSHModel].getName

    override def load(path: String): RandomHyperplanesLSHModel = {
      val metadata = DefaultParamsReader.loadMetadata(path, sc, className)

      val dataPath = new Path(path, "data").toString
      val data = sparkSession.read.parquet(dataPath).select("random_planes").head()
      val random_planes = data.getSeq[Vector](0).toArray
      val model = new RandomHyperplanesLSHModel(metadata.uid, random_planes)

      metadata.getAndSetParams(model)
      model
    }
  }
}